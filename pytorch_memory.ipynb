{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "p_jkTcAsGzNL"
      },
      "outputs": [],
      "source": [
        "! pip install transformers datasets accelerate -qqq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "d7QCi7EKG79n"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IQ6kmqZHhqh",
        "outputId": "12c79a7d-5464-4d87-a14d-2606daa067e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU 사용량:  0.000 GB\n"
          ]
        }
      ],
      "source": [
        "def gpu_utilization():\n",
        "    '''\n",
        "    gpu에 메모리 할당이 얼마나 되어있는지 확인하는 함수\n",
        "    1024 ** 3 을 통해 GB로 만들어줌\n",
        "    '''\n",
        "    if torch.cuda.is_available():\n",
        "        memory = torch.cuda.memory_allocated() / (1024 **3)\n",
        "        print(f\"GPU 사용량: {memory: .3f} GB\")\n",
        "    else:\n",
        "        print(\"GPU\")\n",
        "\n",
        "gpu_utilization()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "u8QEy33RIWfo"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aC0BVwqOJIdb"
      },
      "outputs": [],
      "source": [
        "def load_model_and_tokenizer(model_id):\n",
        "    '''\n",
        "    model과 token을 가져오는 함수\n",
        "    torch_dtype = 'auto'는 gpu에 맞는 타입으로 모델을 변경해줌. T4인 경우 f16으로 변경\n",
        "    device_map : 모델을 gpu를 0번부터 순차적으로 load 하겠다\n",
        "    '''\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map={\"\":0})\n",
        "    if torch.cuda.is_available():\n",
        "        model.to('cuda')\n",
        "\n",
        "    return tokenizer, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pUZgHcE3fVwq"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "model_id = 'EleutherAI/polyglot-ko-1.3b'\n",
        "tokenizer, model = load_model_and_tokenizer(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "416ASjYffmeE",
        "outputId": "ae91e13c-d162-4c88-ac11-1c658e40be09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU 사용량:  2.599 GB\n"
          ]
        }
      ],
      "source": [
        "gpu_utilization()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HHmTUcCThwJ1"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FCV0X_wwzuO0"
      },
      "outputs": [],
      "source": [
        "def estimate_memory_of_gradients(model):\n",
        "    '''\n",
        "    model gradients 메모리 추정 함수\n",
        "\n",
        "    '''\n",
        "    total_memory = 0\n",
        "    for param in model.parameters():\n",
        "        if param.grad is not None:\n",
        "            total_memory += param.grad.nelement() * param.grad.element_size() # nelemnt: 저장된 값의 수, element_size: 데이터 크기\n",
        "    return total_memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "275qgMZS0hnt"
      },
      "outputs": [],
      "source": [
        "def estimate_memory_of_optimizer(optimizer):\n",
        "    '''\n",
        "    optimizer의 메모리 추정 함수\n",
        "    '''\n",
        "    total_memory = 0\n",
        "    for state in optimizer.state.values():\n",
        "        for k, v in state.items():\n",
        "            if torch.is_tensor(v):\n",
        "                total_memory += v.nelement() * v.element_size()\n",
        "    return total_memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kluEHMaz1QV-"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "\n",
        "def make_dummy_dataset():\n",
        "    seq_len, dataset_size = 256, 64\n",
        "    dummy_data = {\n",
        "        'input_ids' : np.random.randint(100, 30000, (dataset_size, seq_len)),\n",
        "        'labels' : np.random.randint(100, 30000, (dataset_size, seq_len)),\n",
        "    }\n",
        "\n",
        "    dataset = Dataset.from_dict(dummy_data)\n",
        "    dataset.set_format(\"pt\")\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nJDKCa5v_bA8"
      },
      "outputs": [],
      "source": [
        "dataset = make_dummy_dataset()\n",
        "batch_size = 4\n",
        "train_dataloader = DataLoader(dataset, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "OF6yLcNlAPT9"
      },
      "outputs": [],
      "source": [
        "def train_model(model, dataloader):\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr = 0.001)\n",
        "    for batch in dataloader:\n",
        "        batch = {k : v.to(model.device) for k, v in batch.items()}\n",
        "\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        gradients_memory = estimate_memory_of_gradients(model)\n",
        "        optimizer_memory = estimate_memory_of_optimizer(optimizer)\n",
        "        gpu_utilization()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        print(f\"옵티마이저 메모리 사용량: {optimizer_memory / (1024 ** 3):.3f} GB\")\n",
        "        print(f\"그래디언트 메모리 사용량: {gradients_memory / (1024 ** 3):.3f} GB\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBzW-pGtQf0L"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqRZO5YKQIvc",
        "outputId": "0ad2cb5e-bc63-431a-cddb-97df2c07debf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU 사용량:  10.586 GB\n",
            "옵티마이저 메모리 사용량: 4.961 GB\n",
            "그래디언트 메모리 사용량: 2.481 GB\n"
          ]
        }
      ],
      "source": [
        "train_model(model, train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "AGcOjZTJQOsg"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "def cleanup():\n",
        "    if 'model' in globals(): # cpu에서 학습에 사용되었던 메모리가 있으면 직접 삭제\n",
        "        del globals()['model']\n",
        "    if 'dataset' in globals():\n",
        "        del globals()['dataset']\n",
        "    gc.collect()  # python interpreter에서 사용하지 않는 객체를 탐지해서 수집\n",
        "    torch.cuda.empty_cache() # gpu 캐시 비우기\n",
        "\n",
        "#globals()를 호출하면 현재 모듈의 전역 변수와 그 값들이 담긴 딕셔너리를 반환합니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uixG8er3Wa6m"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
